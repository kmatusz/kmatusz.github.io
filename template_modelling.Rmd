---
title: "Template for ML modelling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE, message = FALSE)
```
Using machine learning is a highly repetitive process. In this notebook I tried to create unified template to save typing time in my next projects. Most of the time it's as easy as changing input dataset, and in other cases it is also helpful as a reminder of steps to make. 

#### Basics 
```{r}
library(tidyverse)
library(caTools)
library(ROCR)
library(caret)

```

Here we assign our dataset to name *df* and changing the target column name to *TARGET*
```{r}
df <- read.csv("Train.csv", stringsAsFactors = F)
df<-df%>%select(-Loan_ID)
name_target<-"Loan_Status"
names(df)[which(names(df)==name_target)]<-"TARGET"
```

Here is the place for explanatory data analysis. As this process is highly dependent on the dataset, I included only small chunk.
```{r}
df%>%summary()
df$TARGET%>%summary()
```

#### Preprocessing
Now it's time for preprocessing the dataset to suit Caret package methodology. First step is cropping the dataset. Use this step only if you encounter memory usage problems during fitting the model.
```{r}

#index_head<-createDataPartition(df$TARGET, p =0.005, list = F) #experiment with p value, the more data you leave unchaged the better for the accuracy.
#df_small<-df[index_head,]
df_small<-df
```

### Dealing with missing values
There are quite a few approaches to deal with missing values, and the question what to do is highly case-dependent. Here I'm using median imputation for numeric variables and changing NA value to "na" string in text variables. Using this technique you will probably obtain some reasonable baseline to test out another ideas.

```{r}

df_small%>%
  mutate_if(is.numeric,function(x) ifelse(is.na(x), quantile(x, 0.5, na.rm=T)%>%as.numeric(), x))%>%   #numeric variables
  mutate_if(function(x) !is.numeric(x), function(x) ifelse(is.na(x), "na", x))->df_small
```



Next step is using dummy variables to encode strings as numbers. This is versatile approach that will work using every model, but this step isn't always necessary as some models (decision trees for example) can also deal with categorical variables.

```{r}
dummies <- dummyVars(TARGET ~ ., data = df_small)
df_small_dum<-predict(dummies, df_small)%>%as.data.frame()
df_small_dum$TARGET<-as.factor(df_small$TARGET)
df_small<-df_small_dum
rm(df_small_dum)
```

The dataset is almost ready to modelling. Other steps to add would be for example dimensionality reduction using PCA. 


Creating training and test sets:
```{r}
index_train<-createDataPartition(df_small$TARGET, p=0.7, list=F)
training<-df_small[index_train, ]
test<-df_small[-index_train, ]

```

To save execution time good idea is to save preprocessed dataset to a file.
```{r}
save.image("data_preprocessed.Rdata")
#load("data_preprocessed.RData")
```

## Modelling

And now the funniest part. Creating models and evaluation using caret package is a piece of cake. A standard approach would be to define training control (in this case repeatedCV) and use it iteratively using different models to obtain better and better results. Last step is comparing the models using resampling and choosing the winner.
```{r}
library(gbm)
tr_cont<- trainControl(method="repeatedcv", 
                       #add these two lines if you are using AUC as  a metric:
                       #summaryFunction = twoClassSummary, 
                       #classProbs = T, 
                      repeats=2, number=2)

```

#### Model 1- gbm
```{r}
model1<-train(TARGET~. , data= training,
              method="gbm",
              verbose=F,
              #metric="ROC",
              trControl=tr_cont
              )
```

Checking performance and stats of the model
```{r}
model1
#plot(model1)
```

```{r}
varImp(model1)%>%plot()
```

#### Model 2- glm
```{r}
model2<-train(TARGET~. , data= training,
              method="glm", 
              #metric="ROC",
              family=binomial(), #this is a parameter of glm model, to remove in other models
              trControl=tr_cont
              )
```

Checking performance and stats of the model
```{r}
model2
```

```{r}
varImp(model2)%>%plot()
```



... and so on. Once the schema is right, adding new models to test is as easy as copying few lines and changing name of the model.

